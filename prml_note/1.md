Erms：root-mean-square error， 均方根误差
随机变量非线性变换时的概率密度？
期望和方差
高斯分布
极大似然估计是为了求让后验概率最大的参数w
固定高斯分布的其中一个参数，可以发现极大似然估计出来的平均数就是期望，而方差比实际小了，这就是bias偏差，乘一个系数得到无偏估计。
这里假设误差是高斯分布的，可以用极大似然估计求出均值，然后根据均值求出方差。
最大后验就是最大化带正则项的似然估计

增广阵解矩阵方程。
矩阵变换都是线性的，可逆变换对应可逆矩阵
矩阵和他的转置相乘是对称阵
LU分解，对于相同系数的矩阵方程来说可以少计算（不用变换增广阵最后一列）
线性空间和子空间，要满足空间内v1，v2两个向量， cv1+cv2后还在本空间内。
4个子空间，零空间正交行空间，左零空间正交列空间,零空间的维数是非主元列个数a，列空间的则是主元列个数b，这说明a+b=列数，
看是谁的子空间，要看基的元素数量，比如基有5个元素，而基一共有3个向量，那么是R5的子空间。
矩阵乘法本质上是坐标系的变化。
生成集定理：空间由线性无关的向量（基）生成。
列空间由对应的简化阶梯型矩阵主元列生成（简化是通过行等价的，行等价不改变线性相关性），非主元列剔除。
线性变换的和与值域。
观察矩阵列的线性相关性只要看某一列j是否是前面j-1列的线性组合，这个是线性无关的充要条件。
唯一表示定理：向量空间中的向量只能被基以唯一的系数表示，证明可以假设两种表示，但是因为基是线性无关的，基不能组合，所以相减的系数都为0，所以两种系数相同。
坐标映射是一个由向量空间映射到Rn的一对一映射，而且加法和标量乘法封闭，所以也是个线性变换。
列m大于行n的矩阵有m-n个列不是主元列，至少有一列不是主元列的矩阵列向量线性相关，因为一列是0，而行等价于原矩阵。
Rn的基是可逆的，不要想太多非方阵什么的，就是方阵。
生成子空间的基的个数叫做维数，考察R3，基中的一个向量生成的子空间维数为2，两个生成的维数为2。
求行列零空间的基：对于列空间来说，找到行化简为阶梯型后的主元列位置，对应原矩阵的列就能找到列空间的基。
NulA的基要把矩阵行化简到最简然后算出解。
行空间，找行等价的非零行，非零行一眼就能看出来是线性无关的。
为什么行等价变换后不改变行空间？因为行等价变换是可逆线性变换，因此原矩阵和行等价矩阵的行向量互相可以表示，因此互相包含，所以行空间是相等的。
秩等于列空间的维数，生成列空间的基的个数，主元列的个数。
dimColA + dimNulA=n  -> RankA + dimNulA=n。
维数描述的是基，秩描述的是矩阵。
B和C是向量空间V的基，存在一个nxn矩阵P使向量的B基坐标转换成C基坐标，这个矩阵的列是B基在C基中的坐标，P可逆，转换到标准基的矩阵就是B。
dot(Ux,Uy) = dot(x,y):隐含了乘正交阵保留向量的长度，保留两个向量的正交性。
找到z正交W：Rn内任意向量y都可以表示为Rn-1子空间W内一向量和正交于W的一个向量yhat的和（只要证明y-yhat正交于W）。
最佳逼近：找z和yhat，yhat是最接近y的值。
构造单位正交基：用格拉姆施密特法则构造正交基后单位化基向量，假设基为{x1，x2，...，xn}，第一个基为z1=x1，第二个基为z2=x2-x2在x1上的投影，
z1，z2构成子空间W，第三个基z3=x3-x3在W上的投影，以此类推。
QR分解：若A各列线性无关，那么A可以分解成QR,Q是标准正交基，R是上三角可逆矩阵，且对角元素都为正。
证明：用施密特法则找到Q，然后A的各列都可以用QR表示，找到R即可。
二次型的值有正定，负定，不定，半正定，半负定。
二次型的条件优化，最大值为最大的特征值，对应的x是特征值对应的特征向量。加一个条件x和最大的特征向量必须正交，那么可以推出最大值为第二大的特征值，以此类推。
可对角化的充要条件是有n个不同的特征值，n = A列数
充要：每个对称阵都可以正交对角化。
奇异值分解不需要对角化那么特殊的特性，任何AA.T都是对称阵
奇异值等于特征值开方等于Avi的长度，所以奇异值非零代表Avi非0。
A的秩等于非零奇异值个数r，{Av1，...Avr}是ColA的正交基。
乘一个矩阵可以看成基变换，要换成标准基的坐标的话，只要左乘当前基为列组成的矩阵。
2,3阶行列式可以看成标准基上的面积和体积。
行列式是线性变换。
行列式不为0->可逆->求特征值就是求让特征方程行列式为0的lambda。
不同的特征值对应的特征向量线性无关。
PCA：1、去特征间的相关性，通过P矩阵转换X到Y，使协方差为0，根据熵定理得出方差最大的包含信息量最大。
    2、降维，为什么可行?：这里有个前提，数据必须预处理（中心化和缩放）。
    若A可以对角化，经过特征分解后变成Σlambda ui ui.T是累加形式，特征值小的对应的部分可以舍弃掉。

熵：自信息，描述随机变量平均信息量的大小，也就是要有多少信息才能表示这个随机变量总体，随机变量越不确定隐含信息越大。
最大熵就是均匀分布的熵，在先验条件约束下，反应人对事物的无知，无知代表了所有可能性都平等，可以用拉格朗日乘数法求。
为什么26个字符的编码不管概率怎么分布，只要不为0，至少都要有5个比特来表示，
因为语言模型L(x1,x2...)可以是无限长的，所以一般会假设每个字符出现的概率相等，均匀分布，
字符越多，每个字符出现的概率越低，越不能确定出现哪个字符，L的熵越高。
假设随机事件A，发生a1的概率是1/100,a2是1/6,a3是1-1/100-1/6,这个概率分布不是均匀的说明已经有了先验约束，a3概率很大所以A很确定结果，
a3概率变得越小则A越不确定。换个方式，通过先验我知道了a3最大，那么先猜a3，这样比随机分布猜的期望次数会少，剩下的a1，a2均匀分布，又知道
a2比a1大，那么猜a2会比随机猜期望次数少，所以这个分布不均匀的话会比随机分布的熵要小，所以说越是确定（分布）信息越是少熵越大。
联合熵就是联合分布的熵。
条件熵是对Y对随机变量X下的期望熵，对x积Σp(Y)log(p(Y|x))

