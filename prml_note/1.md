Erms：root-mean-square error， 均方根误差
随机变量非线性变换时的概率密度？
期望和方差
高斯分布
极大似然估计是为了求让后验概率最大的参数w
固定高斯分布的其中一个参数，可以发现极大似然估计出来的平均数就是期望，而方差比实际小了，这就是bias偏差，乘一个系数得到无偏估计。
这里假设误差是高斯分布的，可以用极大似然估计求出均值，然后根据均值求出方差。
最大后验就是最大化带正则项的似然估计

增广阵解矩阵方程。
矩阵变换都是线性的，可逆变换对应可逆矩阵
矩阵和他的转置相乘是对称阵
LU分解，对于相同系数的矩阵方程来说可以少计算（不用变换增广阵最后一列）
线性空间和子空间，要满足空间内v1，v2两个向量， cv1+cv2后还在本空间内。
4个子空间，零空间正交行空间，左零空间正交列空间。
矩阵乘法本质上是坐标系的变化。
生成集定理：空间由线性无关的向量（基）生成。
列空间由对应的简化阶梯型矩阵主元列生成（简化是通过行等价的，行等价不改变线性相关性），非主元列剔除。
线性变换的和与值域。
观察矩阵列的线性相关性只要看某一列j是否是前面j-1列的线性组合，这个是线性无关的充要条件。
唯一表示定理：向量空间中的向量只能被基以唯一的系数表示，证明可以假设两种表示，但是因为基是线性无关的，基不能组合，所以相减的系数都为0，所以两种系数相同。
坐标映射是一个由向量空间映射到Rn的一对一映射，而且加法和标量乘法封闭，所以也是个线性变换。



熵：自信息，描述随机变量平均信息量的大小，也就是要有多少信息才能表示这个随机变量总体，随机变量越不确定隐含信息越大。
最大熵就是均匀分布的熵，在先验条件约束下，反应人对事物的无知，无知代表了所有可能性都平等，可以用拉格朗日乘数法求。

为什么26个字符的编码不管概率怎么分布，只要不为0，至少都要有5个比特来表示，
因为语言模型L(x1,x2...)可以是无限长的，所以一般会假设每个字符出现的概率相等，均匀分布，
字符越多，每个字符出现的概率越低，越不能确定出现哪个字符，L的熵越高。

假设随机事件A，发生a1的概率是1/100,a2是1/6,a3是1-1/100-1/6,这个概率分布不是均匀的说明已经有了先验约束，a3概率很大所以A很确定结果，
a3概率变得越小则A越不确定。换个方式，通过先验我知道了a3最大，那么先猜a3，这样比随机分布猜的期望次数会少，剩下的a1，a2均匀分布，又知道
a2比a1大，那么猜a2会比随机猜期望次数少，所以这个分布不均匀的话会比随机分布的熵要小，所以说越是确定（分布）信息越是少熵越大。

联合熵就是联合分布的熵。
条件熵是对Y对随机变量X下的期望熵，对x积Σp(Y)log(p(Y|x))

